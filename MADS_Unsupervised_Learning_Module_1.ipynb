{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_You are currently using **version 1.1** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-machine-learning/resources/bANLa) course resource._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIADS 543 Unsupervised Learning (Week 1): \n",
    "# Dimensionality Reduction and Density Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either of the following is no longer\n",
    "# necessary for matplotlib in notebooks.\n",
    "# The import statement has you covered!\n",
    "\n",
    "# %matplotlib notebook\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress all warnings only when absolutely necessary\n",
    "# Warnings are in place for a reason!\n",
    "import warnings\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "# warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "\n",
    "## Additional imports can be inlcuded here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our sample fruits dataset\n",
    "fruits = pd.read_table(\"fruit_data_with_colors.txt\")\n",
    "X_fruits = fruits[[\"mass\", \"width\", \"height\", \"color_score\"]]\n",
    "y_fruits = fruits[[\"fruit_label\"]] - 1\n",
    "y_fruits = y_fruits.values.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toy example with SVD on rotation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotation_90r = np.matrix([[0, +1],[-1, 0]])\n",
    "rotation_90r = np.array([[0, +1], [-1, 0]])\n",
    "unit_y = np.array([0, 1])\n",
    "np.matmul(rotation_90r, np.transpose(unit_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "\n",
    "U, s, Vh = linalg.svd(rotation_90r)\n",
    "print(\"X:\\n\", rotation_90r)\n",
    "print(\"U:\\n\", U)\n",
    "print(\"Singular values:\\n\", s)\n",
    "print(\"Vh:\\n\", Vh)\n",
    "print(\"U, s, Vh shapes:\", U.shape, s.shape, Vh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstitute the original matrix X exactly by multiplying the factors together.\n",
    "S = np.diag(s)\n",
    "np.allclose(rotation_90r, np.dot(U, np.dot(S, Vh)))\n",
    "np.dot(U, np.dot(S, Vh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toy example with SVD on 7x3 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_3d_matrix = np.transpose(np.matrix([[5, 4, 3, 1, 0, 0 ,0], [0, 1, 1, 1, 0, 6, 7], [2, 0, 0, 1, 2, 4, 6]]))\n",
    "sample_3d_matrix = np.transpose(\n",
    "    np.array([[5, 4, 3, 1, 0, 0, 0], [0, 1, 1, 1, 0, 6, 7], [2, 0, 0, 1, 2, 4, 6]])\n",
    ")\n",
    "U, s, Vh = linalg.svd(sample_3d_matrix, full_matrices=False)\n",
    "np.set_printoptions(precision=2)\n",
    "print(\"X:\\n\", sample_3d_matrix)\n",
    "print(\"U:\\n\", U)\n",
    "print(\"Singular values:\\n\", s)\n",
    "print(\"Vh:\\n\", Vh)\n",
    "print(\"U, s, Vh shapes:\", U.shape, s.shape, Vh.shape)\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import random as sparse_random\n",
    "\n",
    "# from sklearn.random_projection import sparse_random_matrix\n",
    "svd = TruncatedSVD(n_components=2, n_iter=7, random_state=42)\n",
    "svd.fit(sample_3d_matrix)\n",
    "print(svd.explained_variance_ratio_)\n",
    "print(svd.components_)\n",
    "print(svd.singular_values_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example using SVD from scipy.linalg to factor a simple matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "\n",
    "m, n = 3, 5\n",
    "# a = np.matrix([[ 11., 13.,   5.,  1.,  2.], [ 7. , 4. , 6. ,11. , 16.], [16. , 14. , 2. , 2. , 3.]])\n",
    "a = np.array(\n",
    "    [\n",
    "        [11.0, 13.0, 5.0, 1.0, 2.0],\n",
    "        [7.0, 4.0, 6.0, 11.0, 16.0],\n",
    "        [16.0, 14.0, 2.0, 2.0, 3.0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "U, s, Vh = linalg.svd(a)\n",
    "\n",
    "print(\"Shapes of U, s, and Vh:\", U.shape, s.shape, Vh.shape)\n",
    "sigma = np.zeros((m, n))\n",
    "\n",
    "for i in range(min(m, n)):\n",
    "    sigma[i, i] = s[i]\n",
    "\n",
    "a1 = np.dot(U, np.dot(sigma, Vh))\n",
    "print(\"X:\", a)\n",
    "print(\"U:\", U)\n",
    "print(\"sigma:\", sigma)\n",
    "print(\"Vh:\", Vh)\n",
    "print(\"Reconstituted matrix: \", a1)\n",
    "print(\"Close to original matrix?\", np.allclose(a, a1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truncated SVD example using svds from scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "np.random.seed(0)\n",
    "\n",
    "m, n = 4, 4\n",
    "X = np.rint(10 * np.random.random((m, n)))\n",
    "\n",
    "# The output of TruncatedSVD is a wrapper around the following code\n",
    "num_components = 3\n",
    "U, s, V = svds(X, k=num_components)\n",
    "sigma = np.zeros((num_components, num_components))\n",
    "\n",
    "for i in range(min(m, n) - 1):\n",
    "    sigma[i, i] = s[i]\n",
    "\n",
    "print(\"Shapes U, s, V:\", U.shape, s.shape, V.shape)\n",
    "print(\"X original:\\n\", X)\n",
    "print(\"U:\\n\", U)\n",
    "print(\"s:\\n\", s)\n",
    "print(\"sigma:\\n\", sigma)\n",
    "print(\"V:\\n\", V)\n",
    "print(\"---\")\n",
    "X_approx = np.dot(U, np.dot(sigma, V))\n",
    "print(\"X approximation:\\n\", X_approx)\n",
    "print(\"Error of X approximation:\\n\", X_approx - X)\n",
    "print(\"Frobenius norm of X approximation: \", np.linalg.norm(X_approx - X, ord=\"fro\"))\n",
    "print(\"Close to original matrix?\", np.allclose(X, X_approx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Components Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using PCA to find the first two principal components of the breast cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "(X_cancer, y_cancer) = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Before applying PCA, each feature should be centered (zero mean) and with unit variance\n",
    "X_cancer_normalized = StandardScaler().fit(X_cancer).transform(X_cancer)\n",
    "\n",
    "pca = PCA(n_components=2).fit(X_cancer_normalized)\n",
    "X_pca = pca.transform(X_cancer_normalized)\n",
    "print(X_cancer_normalized.shape, X_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### Computing covariance matrix, comparing unnormalized and normalized data\n",
    "#### Here's the covariance of the unnormalized data\n",
    "from sklearn.covariance import empirical_covariance\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sn.heatmap(empirical_covariance(X_cancer))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and here's the covariance of the normalized data\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sn.heatmap(empirical_covariance(X_cancer_normalized))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2).fit(X_cancer_normalized)\n",
    "X_pca = pca.transform(X_cancer_normalized)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sn.heatmap(X_pca)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the PCA-transformed version of the breast cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adspy_shared_utilities import plot_labelled_scatter\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.grid(alpha=0.2)\n",
    "plot_labelled_scatter(X_pca, y_cancer, [\"malignant\", \"benign\"])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### What happens if you forget to normalize the input to PCA?\n",
    "\n",
    "pca_unnormalized = PCA(n_components=2).fit(X_cancer)\n",
    "X_pca_unnormalized = pca_unnormalized.transform(X_cancer)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.grid(alpha=0.2)\n",
    "plot_labelled_scatter(X_pca_unnormalized, y_cancer, [\"malignant\", \"benign\"])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the magnitude of each feature value for the first two principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cancer_pca(pca, top_k=2):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plt.imshow(pca.components_[0:top_k], interpolation=\"none\", cmap=\"plasma\")\n",
    "    feature_names = list(cancer.feature_names)\n",
    "    plt.xticks(\n",
    "        np.arange(-0.0, len(feature_names), 1), feature_names, rotation=90, fontsize=12\n",
    "    )\n",
    "    plt.yticks(np.arange(0.0, 2, 1), [\"First PC\", \"Second PC\"], fontsize=16)\n",
    "    plt.colorbar()\n",
    "\n",
    "\n",
    "plot_cancer_pca(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare PCA with 3 components: regular and sparse\n",
    "\n",
    "# First, regular PCA\n",
    "pca = PCA(n_components=3).fit(X_cancer_normalized)\n",
    "plot_cancer_pca(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now let's try sparse PCA. The solution is sensitive to the choice of sparsity parameter alpha.\n",
    "\n",
    "from sklearn.decomposition import SparsePCA\n",
    "\n",
    "pca_sparse = SparsePCA(n_components=3, alpha=15, random_state=0)\n",
    "pca_sparse.fit(X_cancer_normalized)\n",
    "X_transformed = pca_sparse.transform(X_cancer_normalized)\n",
    "\n",
    "# most values in the components_ are zero (sparsity)\n",
    "percentage = np.sum(pca_sparse.components_[0] == 0) / len(pca_sparse.components_[0])\n",
    "print(\"{:2.2%} of values are zero in Sparse PCA first component.\".format(percentage))\n",
    "plot_cancer_pca(pca_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading a biplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_subset_count = 5\n",
    "pca = PCA(n_components=10).fit(X_cancer_normalized)\n",
    "X_pca = pca.transform(X_cancer_normalized)\n",
    "plot_cancer_pca(pca)\n",
    "feature_names = list(cancer.feature_names)\n",
    "\n",
    "### Feel free to use this routine to plot your own biplots!\n",
    "def biplot(score, coeff, maxdim, pcax, pcay, labels=None):\n",
    "    zoom = 0.5\n",
    "    pca1 = pcax - 1\n",
    "    pca2 = pcay - 1\n",
    "    xs = score[:, pca1]\n",
    "    ys = score[:, pca2]\n",
    "    n = min(coeff.shape[0], maxdim)\n",
    "    width = 2.0 * zoom\n",
    "    scalex = width / (xs.max() - xs.min())\n",
    "    scaley = width / (ys.max() - ys.min())\n",
    "    text_scale_factor = 1.3\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(9, 9)\n",
    "\n",
    "    plt.scatter(xs * scalex, ys * scaley, s=9)\n",
    "    for i in range(n):\n",
    "        plt.arrow(\n",
    "            0,\n",
    "            0,\n",
    "            coeff[i, pca1],\n",
    "            coeff[i, pca2],\n",
    "            color=\"b\",\n",
    "            alpha=0.9,\n",
    "            head_width=0.03 * zoom,\n",
    "        )\n",
    "        if labels is None:\n",
    "            plt.text(\n",
    "                coeff[i, pca1] * text_scale_factor,\n",
    "                coeff[i, pca2] * text_scale_factor,\n",
    "                \"Var\" + str(i + 1),\n",
    "                color=\"g\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "            )\n",
    "        else:\n",
    "            plt.text(\n",
    "                coeff[i, pca1] * text_scale_factor,\n",
    "                coeff[i, pca2] * text_scale_factor,\n",
    "                labels[i],\n",
    "                color=\"g\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "            )\n",
    "\n",
    "    plt.xlim(-zoom, zoom)\n",
    "    plt.ylim(-zoom, zoom)\n",
    "    plt.xlabel(\"PC{}\".format(pcax))\n",
    "    plt.ylabel(\"PC{}\".format(pcay))\n",
    "    plt.grid()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "feature_subset = slice(0, feature_subset_count, 1)\n",
    "\n",
    "biplot(\n",
    "    X_pca,\n",
    "    np.transpose(pca.components_[0:2, feature_subset]),\n",
    "    feature_subset_count,\n",
    "    1,\n",
    "    2,\n",
    "    labels=feature_names[feature_subset],\n",
    ")\n",
    "\n",
    "print(\"explained_variance_ratio:\", pca.explained_variance_ratio_)\n",
    "print(\"sum of explained variance ratios:\", np.sum(pca.explained_variance_ratio_))\n",
    "print(\"singular values:\", pca.singular_values_)\n",
    "\n",
    "# The variances of the PCs are given by the squares of the singular values of X*, divided by n−1.\n",
    "# Since they are the eigenvalues of the (n-1)S matrix where S is the correlation matrix of X\n",
    "print(np.power(pca.singular_values_, 2) / (X_cancer_normalized.shape[0] - 1))\n",
    "print(X_cancer_normalized.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scree plot to find optimal number of principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"eigenvalue\": np.sqrt(pca.explained_variance_),\n",
    "        \"PC\": [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.grid(alpha=0.2)\n",
    "sn.lineplot(x=\"PC\", y=\"eigenvalue\", data=df, color=\"c\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance plot to find optimal number of principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"explained var ratio\": pca.explained_variance_ratio_,\n",
    "        \"PC\": [\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\", \"PC7\", \"PC8\", \"PC9\", \"PC10\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.grid(alpha=0.2)\n",
    "sn.barplot(x=\"PC\", y=\"explained var ratio\", data=df, color=\"c\")\n",
    "plt.tight_layout()\n",
    "\n",
    "np.cumsum(df[\"explained var ratio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the following cell fails, uncomment the follwoing\n",
    "# line to install the pca package. This cell only needs\n",
    "# to be executed once per invocation of the notebook.\n",
    "\n",
    "# !pip install pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example using the 'pca' library in order to use their built-in biplot routine.\n",
    "# Currently not supported in the Coursera environment.\n",
    "# If you don't have this library locally, you can run: pip install pca\n",
    "\n",
    "from pca import pca\n",
    "\n",
    "model = pca(n_components=10)\n",
    "results = model.fit_transform(X_cancer_normalized)\n",
    "\n",
    "## Make biplot with the number of features\n",
    "plt.figure(figsize=(8, 6))\n",
    "fig, ax = model.biplot(n_feat=10, legend=False)\n",
    "# , labels=feature_names[feature_subset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA on the fruit dataset (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from adspy_shared_utilities import plot_labelled_scatter\n",
    "\n",
    "# each feature should be centered (zero mean) and with unit variance\n",
    "X_normalized = StandardScaler().fit(X_fruits).transform(X_fruits)\n",
    "\n",
    "pca = PCA(n_components=2).fit(X_normalized)\n",
    "X_pca = pca.transform(X_normalized)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.grid(alpha=0.2)\n",
    "plot_labelled_scatter(\n",
    "    X_pca,\n",
    "    y_fruits,\n",
    "    [\"apple\", \"mandarin\", \"orange\", \"lemon\"],\n",
    "    \"Fruits Dataset PCA (n_components = 2)\",\n",
    ")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel PCA example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from adspy_shared_utilities import plot_labelled_scatter\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "X, y = make_circles(n_samples=100, noise=0.01, factor=0.3)\n",
    "\n",
    "X = np.row_stack((X, [-2.0, -2.0]))\n",
    "y = np.append(y, 1)\n",
    "\n",
    "# from the adspy_shared_utilities module\n",
    "kpca = KernelPCA(kernel=\"rbf\", gamma=5)\n",
    "X_kpca = kpca.fit_transform(X)\n",
    "\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.grid(alpha=0.2)\n",
    "\n",
    "plot_labelled_scatter(X, y, [\"class 0\", \"class 1\"], title=\"Original data\")\n",
    "\n",
    "plot_labelled_scatter(\n",
    "    X_kpca, y, [\"class 0\", \"class 1\"], title=\"Kernel PCA with radial basis function\"\n",
    ")\n",
    "\n",
    "plot_labelled_scatter(X_pca, y, [\"class 0\", \"class 1\"], title=\"Regular PCA\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifold learning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multidimensional scaling (MDS) on the fruit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adspy_shared_utilities import plot_labelled_scatter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "# each feature should be centered (zero mean) and with unit variance\n",
    "X_fruits_normalized = StandardScaler().fit(X_fruits).transform(X_fruits)\n",
    "\n",
    "mds = MDS(n_components=2, metric=True)\n",
    "X_fruits_mds = mds.fit_transform(X_fruits_normalized)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.grid(alpha=0.2)\n",
    "plot_labelled_scatter(X_fruits_mds, y_fruits, [\"apple\", \"mandarin\", \"orange\", \"lemon\"])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multidimensional scaling (MDS) on the breast cancer dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This example is included here so you can compare it to the results from PCA.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from adspy_shared_utilities import plot_labelled_scatter\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "(X_cancer, y_cancer) = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# each feature should be centered (zero mean) and with unit variance\n",
    "X_normalized = StandardScaler().fit(X_cancer).transform(X_cancer)\n",
    "\n",
    "mds = MDS(n_components=2)\n",
    "\n",
    "X_mds = mds.fit_transform(X_normalized)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.grid(alpha=0.2)\n",
    "plot_labelled_scatter(\n",
    "    X_mds,\n",
    "    y_cancer,\n",
    "    [\"malignant\", \"benign\"],\n",
    "    title=\"Breast Cancer Dataset MDS (n_components = 2)\",\n",
    ")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### t-SNE on the fruit dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This example from the lecture video is included so that you can see how some dimensionality reduction methods may be less successful on some datasets. Here, it doesn't work as well at finding structure in the small fruits dataset, compared to other methods like MDS.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "init = \"random\"\n",
    "learning_rate = 200.0\n",
    "\n",
    "tsne = TSNE(init=init, learning_rate=learning_rate, random_state=0)\n",
    "\n",
    "X_tsne = tsne.fit_transform(X_fruits_normalized)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.grid(alpha=0.2)\n",
    "plot_labelled_scatter(\n",
    "    X_tsne,\n",
    "    y_fruits,\n",
    "    [\"apple\", \"mandarin\", \"orange\", \"lemon\"],\n",
    "    title=\"Fruits dataset t-SNE\",\n",
    ")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE on the breast cancer dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although not shown in the lecture video, this example is included for comparison, showing the results of running t-SNE on the breast cancer dataset.  See the reading \"How to Use t-SNE effectively\" for further details on how the visualizations from t-SNE are affected by specific parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = \"random\"\n",
    "learning_rate = 200.0\n",
    "\n",
    "tsne = TSNE(init=init, learning_rate=learning_rate, random_state=0)\n",
    "\n",
    "X_tsne = tsne.fit_transform(X_normalized)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.grid(alpha=0.2)\n",
    "plot_labelled_scatter(\n",
    "    X_tsne, y_cancer, [\"malignant\", \"benign\"], title=\"Breast cancer dataset t-SNE\"\n",
    ")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_func(x, loc):\n",
    "    if x == 0:\n",
    "        return \"0\"\n",
    "    elif x == 1:\n",
    "        return \"h\"\n",
    "    elif x == -1:\n",
    "        return \"-h\"\n",
    "    else:\n",
    "        return \"%ih\" % x\n",
    "\n",
    "\n",
    "density_param = {\"density\": True}\n",
    "\n",
    "# Plot the progression of histograms to kernels\n",
    "np.random.seed(1)\n",
    "N = 20\n",
    "X = np.concatenate(\n",
    "    (np.random.normal(0, 1, int(0.3 * N)), np.random.normal(5, 1, int(0.7 * N)))\n",
    ")[:, np.newaxis]\n",
    "X_plot = np.linspace(-5, 10, 1000)[:, np.newaxis]\n",
    "bins = np.linspace(-5, 10, 10)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(12, 8))\n",
    "fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "plt.grid(alpha=0.2)\n",
    "\n",
    "# histogram 1\n",
    "ax[0, 0].hist(X[:, 0], bins=bins, fc=\"#AAAAFF\", **density_param)\n",
    "ax[0, 0].text(-3.5, 0.31, \"Histogram\")\n",
    "\n",
    "# histogram 2\n",
    "ax[0, 1].hist(X[:, 0], bins=bins + 0.75, fc=\"#AAAAFF\", **density_param)\n",
    "ax[0, 1].text(-3.5, 0.31, \"Histogram, bins shifted\")\n",
    "\n",
    "# tophat KDE\n",
    "kde = KernelDensity(kernel=\"tophat\", bandwidth=0.75).fit(X)\n",
    "log_dens = kde.score_samples(X_plot)\n",
    "ax[1, 0].fill(X_plot[:, 0], np.exp(log_dens), fc=\"#AAAAFF\")\n",
    "ax[1, 0].text(-3.5, 0.31, \"Tophat Kernel Density\")\n",
    "\n",
    "# Gaussian KDE\n",
    "kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.75).fit(X)\n",
    "log_dens = kde.score_samples(X_plot)\n",
    "ax[1, 1].fill(X_plot[:, 0], np.exp(log_dens), fc=\"#AAAAFF\")\n",
    "ax[1, 1].text(-3.5, 0.31, \"Gaussian Kernel Density\")\n",
    "\n",
    "for axi in ax.ravel():\n",
    "    axi.plot(X[:, 0], np.full(X.shape[0], -0.01), \"+k\")\n",
    "    axi.set_xlim(-4, 9)\n",
    "    axi.set_ylim(-0.02, 0.34)\n",
    "\n",
    "for axi in ax[:, 0]:\n",
    "    axi.set_ylabel(\"Normalized Density\")\n",
    "\n",
    "for axi in ax[1, :]:\n",
    "    axi.set_xlabel(\"x\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel density estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: modify this original code\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Plot a 1D density example\n",
    "N = 100\n",
    "np.random.seed(1)\n",
    "X = np.concatenate(\n",
    "    (np.random.normal(0, 1, int(0.3 * N)), np.random.normal(5, 1, int(0.7 * N)))\n",
    ")[:, np.newaxis]\n",
    "\n",
    "X_plot = np.linspace(-5, 10, 1000)[:, np.newaxis]\n",
    "\n",
    "true_dens = 0.3 * norm(0, 1).pdf(X_plot[:, 0]) + 0.7 * norm(5, 1).pdf(X_plot[:, 0])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.fill(X_plot[:, 0], true_dens, fc=\"black\", alpha=0.2, label=\"input distribution\")\n",
    "\n",
    "for kernel in [\"gaussian\", \"tophat\", \"epanechnikov\"]:\n",
    "    kde = KernelDensity(kernel=kernel, bandwidth=0.5).fit(X)\n",
    "    log_dens = kde.score_samples(X_plot)\n",
    "    ax.plot(X_plot[:, 0], np.exp(log_dens), \"-\", label=\"kernel = '{0}'\".format(kernel))\n",
    "\n",
    "ax.text(6, 0.38, \"N={0} points\".format(N))\n",
    "ax.set_xlim(-4, 9)\n",
    "ax.set_ylim(-0.02, 0.4)\n",
    "plt.grid(alpha=0.2)\n",
    "ax.plot(X[:, 0], -0.005 - 0.01 * np.random.random(X.shape[0]), \"+k\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get probabilities under the estimated density, using score_samples on a toy dataset\n",
    "X_toy = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "outliers = np.array([[-5, -6], [7, 8]])\n",
    "\n",
    "kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.3).fit(X_toy)\n",
    "\n",
    "# note that we call np.exp() since score_samples returns LOG probabilities\n",
    "print(\"Probabilities of training points:\", np.exp(kde.score_samples(X_toy)))\n",
    "print(\"Probabilities of new (outlier) points:\", np.exp(kde.score_samples(outliers)))\n",
    "\n",
    "# sample 3 new points from this density\n",
    "print(\"Sample 3 random points from this density:\\n\", kde.sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the optimal kernel bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "## This uses the X random 1-d data computed in a previous cell.\n",
    "\n",
    "## Note that the score (being optimized) by default is the total log-likehihood of the data\n",
    "## which is what the KernelDensity score(...) methods returns.\n",
    "\n",
    "bandwidths = 10 ** np.linspace(-1, 1, 20)\n",
    "grid = GridSearchCV(\n",
    "    KernelDensity(kernel=\"gaussian\"), {\"bandwidth\": bandwidths}, cv=LeaveOneOut()\n",
    ")\n",
    "grid.fit(X)\n",
    "print(\"Optimal bandwidth:\", grid.best_params_)\n",
    "print(\"Data log-likelihood:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian mixture model example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_pdf.html\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from sklearn import mixture\n",
    "\n",
    "n_samples = 30\n",
    "\n",
    "# generate random sample, two components\n",
    "np.random.seed(0)\n",
    "\n",
    "# generate spherical data centered on (20, 20)\n",
    "shifted_gaussian = np.random.randn(n_samples, 2) + np.array([20, 20])\n",
    "\n",
    "# add an outlier point\n",
    "shifted_gaussian = np.vstack([shifted_gaussian, [40, 20]])\n",
    "\n",
    "# generate zero centered stretched Gaussian data\n",
    "C = np.array([[0.0, -0.7], [3.5, 0.7]])\n",
    "stretched_gaussian = np.dot(np.random.randn(n_samples, 2), C)\n",
    "\n",
    "# concatenate the two datasets into the final training set\n",
    "X_train = np.vstack([shifted_gaussian, stretched_gaussian])\n",
    "\n",
    "# fit a Gaussian Mixture Model with two components\n",
    "clf = mixture.GaussianMixture(n_components=2, covariance_type=\"full\")\n",
    "clf.fit(X_train)\n",
    "\n",
    "# display predicted scores by the model as a contour plot\n",
    "x = np.linspace(-20.0, 50.0)\n",
    "y = np.linspace(-20.0, 50.0)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "XX = np.array([X.ravel(), Y.ravel()]).T\n",
    "Z = -clf.score_samples(XX)\n",
    "Z = Z.reshape(X.shape)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"Negative log-likelihood predicted by a GMM\")\n",
    "\n",
    "CS = plt.contour(\n",
    "    X, Y, Z, norm=LogNorm(vmin=1.0, vmax=1000.0), levels=np.logspace(0, 3, 10)\n",
    ")\n",
    "CB = plt.colorbar(CS, shrink=0.8)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], 10)\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
